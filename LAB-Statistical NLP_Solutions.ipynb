{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0zDuRecXzEtr"
   },
   "source": [
    "# Text classification using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMPlEJhHzb6P"
   },
   "source": [
    "### 1. Load the dataset from sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fe-B59u3zHNb"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRrMemVQzbHU"
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-sZX0UbJzmg5"
   },
   "source": [
    "### 2. Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CITr_5aXziJ2"
   },
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcESc5QXzr6p"
   },
   "source": [
    "### 3. Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysInblUMzpvl"
   },
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DriL2yZ50DQq"
   },
   "source": [
    "###  a.  You can access the values for the target variable using .target attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlUuai99z1hX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  b. You can access the name of the class in the target variable with .target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEKzaDfSz5E-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clBMKHzC0_N1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n',\n",
       " \"From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\nSubject: help: Splitting a trimming region along a mesh \\nOrganization: University Of Kentucky, Dept. of Math Sciences\\nLines: 28\\n\\n\\n\\n\\tHi,\\n\\n\\tI have a problem, I hope some of the 'gurus' can help me solve.\\n\\n\\tBackground of the problem:\\n\\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \\n\\tmapping of a 3d Bezier patch into 2d. The area in this domain\\n\\twhich is inside a trimming loop had to be rendered. The trimming\\n\\tloop is a set of 2d Bezier curve segments.\\n\\tFor the sake of notation: the mesh is made up of cells.\\n\\n\\tMy problem is this :\\n\\tThe trimming area has to be split up into individual smaller\\n\\tcells bounded by the trimming curve segments. If a cell\\n\\tis wholly inside the area...then it is output as a whole ,\\n\\telse it is trivially rejected. \\n\\n\\tDoes any body know how thiss can be done, or is there any algo. \\n\\tsomewhere for doing this.\\n\\n\\tAny help would be appreciated.\\n\\n\\tThanks, \\n\\tAni.\\n-- \\nTo get irritated is human, to stay cool, divine.\\n\",\n",
       " \"From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSubject: Re: harrassed at work, could use some prayers\\nOrganization: =CSE Dept., U.C. San Diego\\nLines: 63\\n\\n(Well, I'll email also, but this may apply to other people, so\\nI'll post also.)\\n\\n>I've been working at this company for eight years in various\\n>engineering jobs.  I'm female.  Yesterday I counted and realized that\\n>on seven different occasions I've been sexually harrassed at this\\n>company.\\n\\n>I dreaded coming back to work today.  What if my boss comes in to ask\\n>me some kind of question...\\n\\nYour boss should be the person bring these problems to.  If he/she\\ndoes not seem to take any action, keep going up higher and higher.\\nSexual harrassment does not need to be tolerated, and it can be an\\nenormous emotional support to discuss this with someone and know that\\nthey are trying to do something about it.  If you feel you can not\\ndiscuss this with your boss, perhaps your company has a personnel\\ndepartment that can work for you while preserving your privacy.  Most\\ncompanies will want to deal with this problem because constant anxiety\\ndoes seriously affect how effectively employees do their jobs.\\n\\nIt is unclear from your letter if you have done this or not.  It is\\nnot inconceivable that management remains ignorant of employee\\nproblems/strife even after eight years (it's a miracle if they do\\nnotice).  Perhaps your manager did not bring to the attention of\\nhigher ups?  If the company indeed does seem to want to ignore the\\nentire problem, there may be a state agency willing to fight with\\nyou.  (check with a lawyer, a women's resource center, etc to find out)\\n\\nYou may also want to discuss this with your paster, priest, husband,\\netc.  That is, someone you know will not be judgemental and that is\\nsupportive, comforting, etc.  This will bring a lot of healing.\\n\\n>So I returned at 11:25, only to find that ever single\\n>person had already left for lunch.  They left at 11:15 or so.  No one\\n>could be bothered to call me at the other building, even though my\\n>number was posted.\\n\\nThis happens to a lot of people.  Honest.  I believe it may seem\\nto be due to gross insensitivity because of the feelings you are\\ngoing through.  People in offices tend to be more insensitive while\\nworking than they normally are (maybe it's the hustle or stress or...)\\nI've had this happen to me a lot, often because they didn't realize\\nmy car was broken, etc.  Then they will come back and wonder why I\\ndidn't want to go (this would tend to make me stop being angry at\\nbeing ignored and make me laugh).  Once, we went off without our\\nboss, who was paying for the lunch :-)\\n\\n>For this\\n>reason I hope good Mr. Moderator allows me this latest indulgence.\\n\\nWell, if you can't turn to the computer for support, what would\\nwe do?  (signs of the computer age :-)\\n\\nIn closing, please don't let the hateful actions of a single person\\nharm you.  They are doing it because they are still the playground\\nbully and enjoy seeing the hurt they cause.  And you should not\\naccept the opinions of an imbecile that you are worthless - much\\nwiser people hold you in great esteem.\\n-- \\nDarin Johnson\\ndjohnson@ucsd.edu\\n  - Luxury!  In MY day, we had to make do with 5 bytes of swap...\\n\",\n",
       " 'From: s0612596@let.rug.nl (M.M. Zwart)\\nSubject: catholic church poland\\nOrganization: Faculteit der Letteren, Rijksuniversiteit Groningen, NL\\nLines: 10\\n\\nHello,\\n\\nI\\'m writing a paper on the role of the catholic church in Poland after 1989. \\nCan anyone tell me more about this, or fill me in on recent books/articles(\\nin english, german or french). Most important for me is the role of the \\nchurch concerning the abortion-law, religious education at schools,\\nbirth-control and the relation church-state(government). Thanx,\\n\\n                                                 Masja,\\n\"M.M.Zwart\"<s0612596@let.rug.nl>\\n',\n",
       " 'From: stanly@grok11.columbiasc.ncr.com (stanly)\\nSubject: Re: Elder Brother\\nOrganization: NCR Corp., Columbia SC\\nLines: 15\\n\\nIn article <Apr.8.00.57.41.1993.28246@athos.rutgers.edu> REXLEX@fnal.gov writes:\\n>In article <Apr.7.01.56.56.1993.22824@athos.rutgers.edu> shrum@hpfcso.fc.hp.com\\n>Matt. 22:9-14 \\'Go therefore to the main highways, and as many as you find\\n>there, invite to the wedding feast.\\'...\\n\\n>hmmmmmm.  Sounds like your theology and Christ\\'s are at odds. Which one am I \\n>to believe?\\n\\nIn this parable, Jesus tells the parable of the wedding feast. \"The kingdom\\nof heaven is like unto a certain king which made a marriage for his son\".\\nSo the wedding clothes were customary,  and \"given\" to those who \"chose\" to\\nattend.  This man \"refused\" to wear the clothes.  The wedding clothes are\\nequalivant to the \"clothes of righteousness\".  When Jesus died for our sins,\\nthose \"clothes\" were then provided.  Like that man, it is our decision to\\nput the clothes on.\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTz4EaN_1WGc"
   },
   "source": [
    "### 4.  Now with dependent and independent data available for both train and test datasets, using TfidfVectorizer fit and transform the training data and test data and get the tfidf features for both\n",
    "\n",
    "Hint: Use \".fit_transform\" on Train set and \".transform\" on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5G477f81C0Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize vectorizer\n",
    "vect = TfidfVectorizer(ngram_range=(1,2),stop_words='english', min_df = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and transform the training set\n",
    "twenty_train_vec = vect.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the test set\n",
    "twenty_test_vec = vect.transform(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2257, 2543), (1502, 2543))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train_vec.shape, twenty_test_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tp_fDINJ1t4L"
   },
   "source": [
    "### 5. Use logisticRegression with tfidf features as input and targets as output and train the model and report the train and test accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THlN2b5d1yQp"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(twenty_train_vec, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions with train data\n",
    "pred_train = logreg.predict(twenty_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions with test data\n",
    "pred_test = logreg.predict(twenty_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9898094816127603"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions eith train data\n",
    "from sklearn import metrics\n",
    "\n",
    "metrics.accuracy_score(twenty_train.target, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8854860186418109"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate accuracy of class predictions eith test data\n",
    "metrics.accuracy_score(twenty_test.target, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FU-HwvIdH0M-"
   },
   "source": [
    "## Sentiment analysis <br> \n",
    "\n",
    "The objective of this problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
    "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAQDiZHRH0M_"
   },
   "source": [
    "### 6. Read the dataset (tweets.csv) and drop the NA's while reading the dataset\n",
    "\n",
    "Hint: pd.read_csv('./tweets.csv',encoding = \"ISO-8859-1\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eXGIe-SH0NA"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('tweets.csv',encoding = \"ISO-8859-1\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3291, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWeWe1eJH0NF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_text', 'emotion_in_tweet_is_directed_at',\n",
       "       'is_there_an_emotion_directed_at_a_brand_or_product'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPJvTjefH0NI"
   },
   "source": [
    "### 7. Preprocess the text and add the preprocessed text in a column with name `text` in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iec5s9gH0NI"
   },
   "outputs": [],
   "source": [
    "import string, re\n",
    "from nltk import word_tokenize \n",
    "def preprocess(text):\n",
    "    try:\n",
    "        # Check characters to see if they are in punctuation\n",
    "        nopunc = [char for char in text if char not in string.punctuation]\n",
    "        # Join the characters again to form the string.\n",
    "        nopunc = ''.join(nopunc)\n",
    "        # convert text to lower-case\n",
    "        nopunc = nopunc.lower()\n",
    "        # remove URLs\n",
    "        nopunc = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', nopunc)\n",
    "        nopunc = re.sub(r'http\\S+', '', nopunc)\n",
    "        # remove usernames\n",
    "        nopunc = re.sub('@[^\\s]+', '', nopunc)\n",
    "        # remove the # in #hashtag\n",
    "        nopunc = re.sub(r'#([^\\s]+)', r'\\1', nopunc)\n",
    "        return ''.join(nopunc)\n",
    "    except Exception as e:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQSmqA-vH0NT"
   },
   "outputs": [],
   "source": [
    "data['text'] = [preprocess(text) for text in data.tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kX-WoJDH0NV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3291, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>wesley83 i have a 3g iphone after 3 hrs tweeti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>jessedee know about fludapp  awesome ipadiphon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>swonderlin can not wait for ipad 2 also they s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                   Negative emotion   \n",
       "1                                   Positive emotion   \n",
       "2                                   Positive emotion   \n",
       "\n",
       "                                                text  \n",
       "0  wesley83 i have a 3g iphone after 3 hrs tweeti...  \n",
       "1  jessedee know about fludapp  awesome ipadiphon...  \n",
       "2  swonderlin can not wait for ipad 2 also they s...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGWB3P2WH0NY"
   },
   "source": [
    "### 8. Consider only rows having a Positive or Negative emotion and remove other rows from the dataframe.\n",
    "\n",
    "Hint: Use df = df[(df[\"col_name\"] == \"Positive emotion\") OR (df[\"col_name\"] == \"Negative emotion\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdgA_8N2H0NY"
   },
   "outputs": [],
   "source": [
    "data = data[(data[\"is_there_an_emotion_directed_at_a_brand_or_product\"]==\"Positive emotion\") | \n",
    "            (data[\"is_there_an_emotion_directed_at_a_brand_or_product\"]==\"Negative emotion\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Jlu-reIH0Na"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3191, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>wesley83 i have a 3g iphone after 3 hrs tweeti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>jessedee know about fludapp  awesome ipadiphon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>swonderlin can not wait for ipad 2 also they s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>sxsw i hope this years festival isnt as crashy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>sxtxstate great stuff on fri sxsw marissa maye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                   Negative emotion   \n",
       "1                                   Positive emotion   \n",
       "2                                   Positive emotion   \n",
       "3                                   Negative emotion   \n",
       "4                                   Positive emotion   \n",
       "\n",
       "                                                text  \n",
       "0  wesley83 i have a 3g iphone after 3 hrs tweeti...  \n",
       "1  jessedee know about fludapp  awesome ipadiphon...  \n",
       "2  swonderlin can not wait for ipad 2 also they s...  \n",
       "3  sxsw i hope this years festival isnt as crashy...  \n",
       "4  sxtxstate great stuff on fri sxsw marissa maye...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive emotion    2672\n",
       "Negative emotion     519\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"is_there_an_emotion_directed_at_a_brand_or_product\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SotCRvkDH0Nf"
   },
   "source": [
    "### 9. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
    "\n",
    "Hint: Perfrom fit (\".fit\") and transformation(\".transform\") for whole data, later will do CountVectorizer \"fit_transform\" and \"transform\" for train and test separately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcbkY4sgH0Ng"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2),stop_words='english', min_df=0, max_df=1.0)\n",
    "#vect = CountVectorizer(ngram_range=(1,2), min_df=0, max_df=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyXtZGr-H0Nl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3191, 23945)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(data['text'])\n",
    "data_vec = vect.transform(data['text'])\n",
    "data_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4LUM-XPH0Nn"
   },
   "outputs": [],
   "source": [
    "data_vec_df = pd.DataFrame(data_vec.toarray())\n",
    "data_vec_df.columns = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>02</th>\n",
       "      <th>02 symbian</th>\n",
       "      <th>03</th>\n",
       "      <th>03 blackberry</th>\n",
       "      <th>0310</th>\n",
       "      <th>0310 weve</th>\n",
       "      <th>10</th>\n",
       "      <th>10 attendees</th>\n",
       "      <th>10 dangerous</th>\n",
       "      <th>10 hot</th>\n",
       "      <th>...</th>\n",
       "      <th>ûó sxsw</th>\n",
       "      <th>ûó theft</th>\n",
       "      <th>ûójust</th>\n",
       "      <th>ûójust macbooks</th>\n",
       "      <th>ûólewis</th>\n",
       "      <th>ûólewis carroll</th>\n",
       "      <th>ûómention</th>\n",
       "      <th>ûómention connectedbrands</th>\n",
       "      <th>ûóthe</th>\n",
       "      <th>ûóthe right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23945 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   02  02 symbian  03  03 blackberry  0310  0310 weve  10  10 attendees  \\\n",
       "0   0           0   0              0     0          0   0             0   \n",
       "1   0           0   0              0     0          0   0             0   \n",
       "2   0           0   0              0     0          0   0             0   \n",
       "3   0           0   0              0     0          0   0             0   \n",
       "4   0           0   0              0     0          0   0             0   \n",
       "\n",
       "   10 dangerous  10 hot  ...  ûó sxsw  ûó theft  ûójust  ûójust macbooks  \\\n",
       "0             0       0  ...        0         0       0                0   \n",
       "1             0       0  ...        0         0       0                0   \n",
       "2             0       0  ...        0         0       0                0   \n",
       "3             0       0  ...        0         0       0                0   \n",
       "4             0       0  ...        0         0       0                0   \n",
       "\n",
       "   ûólewis  ûólewis carroll  ûómention  ûómention connectedbrands  ûóthe  \\\n",
       "0        0                0          0                          0      0   \n",
       "1        0                0          0                          0      0   \n",
       "2        0                0          0                          0      0   \n",
       "3        0                0          0                          0      0   \n",
       "4        0                0          0                          0      0   \n",
       "\n",
       "   ûóthe right  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "\n",
       "[5 rows x 23945 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pxd5fSHH0Nt"
   },
   "source": [
    "### 10. Find number of different words in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Tip: To see all available functions for an Object use dir and use appropriate function to find number of different words in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1DQ2LdNH0Nu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_char_ngrams',\n",
       " '_char_wb_ngrams',\n",
       " '_check_stop_words_consistency',\n",
       " '_check_vocabulary',\n",
       " '_count_vocab',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_limit_features',\n",
       " '_more_tags',\n",
       " '_sort_features',\n",
       " '_stop_words_id',\n",
       " '_validate_custom_analyzer',\n",
       " '_validate_params',\n",
       " '_validate_vocabulary',\n",
       " '_warn_for_unused_params',\n",
       " '_white_spaces',\n",
       " '_word_ngrams',\n",
       " 'analyzer',\n",
       " 'binary',\n",
       " 'build_analyzer',\n",
       " 'build_preprocessor',\n",
       " 'build_tokenizer',\n",
       " 'decode',\n",
       " 'decode_error',\n",
       " 'dtype',\n",
       " 'encoding',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'fixed_vocabulary_',\n",
       " 'get_feature_names',\n",
       " 'get_params',\n",
       " 'get_stop_words',\n",
       " 'input',\n",
       " 'inverse_transform',\n",
       " 'lowercase',\n",
       " 'max_df',\n",
       " 'max_features',\n",
       " 'min_df',\n",
       " 'ngram_range',\n",
       " 'preprocessor',\n",
       " 'set_params',\n",
       " 'stop_words',\n",
       " 'stop_words_',\n",
       " 'strip_accents',\n",
       " 'token_pattern',\n",
       " 'tokenizer',\n",
       " 'transform',\n",
       " 'vocabulary',\n",
       " 'vocabulary_']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIdZYxJtH0Nq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wesley83': 23017,\n",
       " '3g': 300,\n",
       " 'iphone': 10511,\n",
       " 'hrs': 9272,\n",
       " 'tweeting': 21951,\n",
       " 'riseaustin': 17466,\n",
       " 'dead': 4907,\n",
       " 'need': 14269,\n",
       " 'upgrade': 22197,\n",
       " 'plugin': 15836,\n",
       " 'stations': 19084,\n",
       " 'sxsw': 19739,\n",
       " 'wesley83 3g': 23018,\n",
       " '3g iphone': 307,\n",
       " 'iphone hrs': 10617,\n",
       " 'hrs tweeting': 9273,\n",
       " 'tweeting riseaustin': 21955,\n",
       " 'riseaustin dead': 17467,\n",
       " 'dead need': 4909,\n",
       " 'need upgrade': 14309,\n",
       " 'upgrade plugin': 22201,\n",
       " 'plugin stations': 15838,\n",
       " 'stations sxsw': 19085,\n",
       " 'jessedee': 10931,\n",
       " 'know': 11313,\n",
       " 'fludapp': 6884,\n",
       " 'awesome': 2130,\n",
       " 'ipadiphone': 10457,\n",
       " 'app': 1192,\n",
       " 'youll': 23709,\n",
       " 'likely': 11871,\n",
       " 'appreciate': 1641,\n",
       " 'design': 5083,\n",
       " 'theyre': 21135,\n",
       " 'giving': 7602,\n",
       " 'free': 7073,\n",
       " 'ts': 21840,\n",
       " 'jessedee know': 10932,\n",
       " 'know fludapp': 11317,\n",
       " 'fludapp awesome': 6885,\n",
       " 'awesome ipadiphone': 2148,\n",
       " 'ipadiphone app': 10458,\n",
       " 'app youll': 1360,\n",
       " 'youll likely': 23714,\n",
       " 'likely appreciate': 11872,\n",
       " 'appreciate design': 1642,\n",
       " 'design theyre': 5110,\n",
       " 'theyre giving': 21139,\n",
       " 'giving free': 7605,\n",
       " 'free ts': 7114,\n",
       " 'ts sxsw': 21842,\n",
       " 'swonderlin': 19721,\n",
       " 'wait': 22656,\n",
       " 'ipad': 9958,\n",
       " 'sale': 17676,\n",
       " 'swonderlin wait': 19722,\n",
       " 'wait ipad': 22667,\n",
       " 'ipad sale': 10261,\n",
       " 'sale sxsw': 17682,\n",
       " 'hope': 9142,\n",
       " 'years': 23629,\n",
       " 'festival': 6688,\n",
       " 'isnt': 10797,\n",
       " 'crashy': 4565,\n",
       " 'sxsw hope': 20015,\n",
       " 'hope years': 9157,\n",
       " 'years festival': 23635,\n",
       " 'festival isnt': 6693,\n",
       " 'isnt crashy': 10801,\n",
       " 'crashy years': 4566,\n",
       " 'years iphone': 23636,\n",
       " 'iphone app': 10526,\n",
       " 'app sxsw': 1332,\n",
       " 'sxtxstate': 20577,\n",
       " 'great': 8248,\n",
       " 'stuff': 19448,\n",
       " 'fri': 7127,\n",
       " 'marissa': 12889,\n",
       " 'mayer': 12989,\n",
       " 'google': 7830,\n",
       " 'tim': 21331,\n",
       " 'oreilly': 14966,\n",
       " 'tech': 20816,\n",
       " 'booksconferences': 2834,\n",
       " 'amp': 897,\n",
       " 'matt': 12970,\n",
       " 'mullenweg': 14153,\n",
       " 'wordpress': 23343,\n",
       " 'sxtxstate great': 20578,\n",
       " 'great stuff': 8313,\n",
       " 'stuff fri': 19453,\n",
       " 'fri sxsw': 7129,\n",
       " 'sxsw marissa': 20110,\n",
       " 'marissa mayer': 12890,\n",
       " 'mayer google': 13000,\n",
       " 'google tim': 8042,\n",
       " 'tim oreilly': 21334,\n",
       " 'oreilly tech': 14968,\n",
       " 'tech booksconferences': 20819,\n",
       " 'booksconferences amp': 2835,\n",
       " 'amp matt': 949,\n",
       " 'matt mullenweg': 12971,\n",
       " 'mullenweg wordpress': 14155,\n",
       " 'just': 11030,\n",
       " 'starting': 19058,\n",
       " 'ctia': 4687,\n",
       " 'corner': 4451,\n",
       " 'googleio': 8087,\n",
       " 'hop': 9139,\n",
       " 'skip': 18531,\n",
       " 'jump': 11024,\n",
       " 'good': 7755,\n",
       " 'time': 21340,\n",
       " 'android': 1020,\n",
       " 'fan': 6468,\n",
       " 'sxsw just': 20056,\n",
       " 'just starting': 11125,\n",
       " 'starting ctia': 19059,\n",
       " 'ctia corner': 4688,\n",
       " 'corner googleio': 4453,\n",
       " 'googleio hop': 8088,\n",
       " 'hop skip': 9141,\n",
       " 'skip jump': 18533,\n",
       " 'jump good': 11025,\n",
       " 'good time': 7809,\n",
       " 'time android': 21341,\n",
       " 'android fan': 1045,\n",
       " 'beautifully': 2439,\n",
       " 'smart': 18591,\n",
       " 'simple': 18452,\n",
       " 'idea': 9360,\n",
       " 'rt': 17577,\n",
       " 'madebymany': 12635,\n",
       " 'thenextweb': 21098,\n",
       " 'wrote': 23519,\n",
       " 'hollergram': 9075,\n",
       " 'beautifully smart': 2440,\n",
       " 'smart simple': 18613,\n",
       " 'simple idea': 18453,\n",
       " 'idea rt': 9372,\n",
       " 'rt madebymany': 17586,\n",
       " 'madebymany thenextweb': 12638,\n",
       " 'thenextweb wrote': 21099,\n",
       " 'wrote hollergram': 23520,\n",
       " 'hollergram ipad': 9077,\n",
       " 'ipad app': 9981,\n",
       " 'counting': 4482,\n",
       " 'days': 4893,\n",
       " 'plus': 15841,\n",
       " 'strong': 19422,\n",
       " 'canadian': 3320,\n",
       " 'dollar': 5540,\n",
       " 'means': 13037,\n",
       " 'stock': 19136,\n",
       " 'apple': 1394,\n",
       " 'gear': 7404,\n",
       " 'counting days': 4483,\n",
       " 'days sxsw': 4903,\n",
       " 'sxsw plus': 20192,\n",
       " 'plus strong': 15847,\n",
       " 'strong canadian': 19423,\n",
       " 'canadian dollar': 3321,\n",
       " 'dollar means': 5541,\n",
       " 'means stock': 13042,\n",
       " 'stock apple': 19137,\n",
       " 'apple gear': 1452,\n",
       " 'excited': 6242,\n",
       " 'meet': 13071,\n",
       " 'samsungmobileus': 17706,\n",
       " 'sprint': 18982,\n",
       " 'galaxy': 7321,\n",
       " 'running': 17636,\n",
       " '21': 170,\n",
       " 'fail': 6434,\n",
       " 'excited meet': 6250,\n",
       " 'meet samsungmobileus': 13078,\n",
       " 'samsungmobileus sxsw': 17707,\n",
       " 'sxsw sprint': 20310,\n",
       " 'sprint galaxy': 18983,\n",
       " 'galaxy running': 7322,\n",
       " 'running android': 17637,\n",
       " 'android 21': 1022,\n",
       " '21 fail': 171,\n",
       " 'start': 19039,\n",
       " 'impromptu': 9583,\n",
       " 'parties': 15217,\n",
       " 'hurricaneparty': 9321,\n",
       " 'til': 21323,\n",
       " 'comes': 4024,\n",
       " 'amp start': 975,\n",
       " 'start impromptu': 19043,\n",
       " 'impromptu parties': 9585,\n",
       " 'parties sxsw': 15229,\n",
       " 'sxsw hurricaneparty': 20020,\n",
       " 'hurricaneparty wait': 9322,\n",
       " 'wait til': 22681,\n",
       " 'til android': 21324,\n",
       " 'android app': 1027,\n",
       " 'app comes': 1219,\n",
       " 'foursquare': 7036,\n",
       " 'ups': 22211,\n",
       " 'game': 7326,\n",
       " 'prefer': 16046,\n",
       " 'gowalla': 8179,\n",
       " 'far': 6503,\n",
       " 'best': 2502,\n",
       " 'looking': 12352,\n",
       " 'date': 4826,\n",
       " 'foursquare ups': 7047,\n",
       " 'ups game': 22212,\n",
       " 'game just': 7337,\n",
       " 'just time': 11133,\n",
       " 'time sxsw': 21388,\n",
       " 'sxsw prefer': 20209,\n",
       " 'prefer gowalla': 16048,\n",
       " 'gowalla far': 8184,\n",
       " 'far best': 6505,\n",
       " 'best looking': 2516,\n",
       " 'looking android': 12353,\n",
       " 'app date': 1225,\n",
       " 'gotta': 8166,\n",
       " 'love': 12450,\n",
       " 'calendar': 3255,\n",
       " 'featuring': 6633,\n",
       " 'cases': 3401,\n",
       " 'check': 3609,\n",
       " 'hamsandwich': 8594,\n",
       " 'ischafer': 10795,\n",
       " 'gt': 8422,\n",
       " 'gotta love': 8171,\n",
       " 'love sxsw': 12484,\n",
       " 'sxsw google': 19971,\n",
       " 'google calendar': 7864,\n",
       " 'calendar featuring': 3256,\n",
       " 'featuring parties': 6636,\n",
       " 'parties cases': 15220,\n",
       " 'cases check': 3402,\n",
       " 'check rt': 3645,\n",
       " 'rt hamsandwich': 17584,\n",
       " 'hamsandwich ischafer': 8595,\n",
       " 'ischafer gt': 10796,\n",
       " 'great sxsw': 8315,\n",
       " 'sxsw ipad': 20042,\n",
       " 'app madebymany': 1292,\n",
       " 'haha': 8567,\n",
       " 'awesomely': 2177,\n",
       " 'rad': 16833,\n",
       " 'haha awesomely': 8568,\n",
       " 'awesomely rad': 2178,\n",
       " 'rad ipad': 16835,\n",
       " 'madebymany hollergram': 12637,\n",
       " 'hollergram sxsw': 9078,\n",
       " 'noticed': 14652,\n",
       " 'dst': 5786,\n",
       " 'coming': 4036,\n",
       " 'weekend': 22977,\n",
       " 'users': 22317,\n",
       " 'hour': 9228,\n",
       " 'late': 11443,\n",
       " 'come': 3988,\n",
       " 'sunday': 19557,\n",
       " 'morning': 14094,\n",
       " 'just noticed': 11102,\n",
       " 'noticed dst': 14653,\n",
       " 'dst coming': 5787,\n",
       " 'coming weekend': 4056,\n",
       " 'weekend iphone': 22981,\n",
       " 'iphone users': 10709,\n",
       " 'users hour': 22328,\n",
       " 'hour late': 9232,\n",
       " 'late sxsw': 11452,\n",
       " 'sxsw come': 19855,\n",
       " 'come sunday': 4017,\n",
       " 'sunday morning': 19560,\n",
       " 'morning sxsw': 14102,\n",
       " 'sxsw iphone': 20046,\n",
       " 'added': 648,\n",
       " 'flights': 6845,\n",
       " 'planely': 15744,\n",
       " 'matching': 12963,\n",
       " 'people': 15417,\n",
       " 'planesairports': 15746,\n",
       " 'downloaded': 5669,\n",
       " 'klm': 11300,\n",
       " 'nicely': 14556,\n",
       " 'just added': 11033,\n",
       " 'added sxsw': 652,\n",
       " 'sxsw flights': 19936,\n",
       " 'flights planely': 6851,\n",
       " 'planely matching': 15745,\n",
       " 'matching people': 12965,\n",
       " 'people planesairports': 15445,\n",
       " 'planesairports downloaded': 15747,\n",
       " 'downloaded klm': 5673,\n",
       " 'klm iphone': 11301,\n",
       " 'app nicely': 1304,\n",
       " 'malbonster': 12752,\n",
       " 'lovely': 12500,\n",
       " 'review': 17368,\n",
       " 'forbes': 6974,\n",
       " 'holler': 9073,\n",
       " 'gram': 8215,\n",
       " 'sxsw app': 19777,\n",
       " 'app rt': 1320,\n",
       " 'rt malbonster': 17588,\n",
       " 'malbonster lovely': 12753,\n",
       " 'lovely review': 12503,\n",
       " 'review forbes': 17370,\n",
       " 'forbes sxsw': 6976,\n",
       " 'app holler': 1266,\n",
       " 'holler gram': 9074,\n",
       " 'buy': 3184,\n",
       " 'ipad2': 10366,\n",
       " 'im': 9430,\n",
       " 'austin': 1935,\n",
       " 'sure': 19606,\n",
       " 'ill': 9406,\n",
       " 'store': 19174,\n",
       " 'need buy': 14278,\n",
       " 'buy ipad2': 3190,\n",
       " 'ipad2 im': 10399,\n",
       " 'im austin': 9434,\n",
       " 'austin sxsw': 2013,\n",
       " 'sxsw sure': 20331,\n",
       " 'sure ill': 19611,\n",
       " 'ill need': 9416,\n",
       " 'need austin': 14273,\n",
       " 'austin apple': 1940,\n",
       " 'apple store': 1556,\n",
       " 'oh': 14775,\n",
       " 'god': 7645,\n",
       " 'pure': 16402,\n",
       " 'unadulterated': 22067,\n",
       " 'easier': 5864,\n",
       " 'browse': 3057,\n",
       " 'events': 6168,\n",
       " 'website': 22943,\n",
       " 'oh god': 14779,\n",
       " 'god sxsw': 7654,\n",
       " 'app ipad': 1271,\n",
       " 'ipad pure': 10235,\n",
       " 'pure unadulterated': 16407,\n",
       " 'unadulterated awesome': 22068,\n",
       " 'awesome easier': 2138,\n",
       " 'easier browse': 5865,\n",
       " 'browse events': 3058,\n",
       " 'events ipad': 6172,\n",
       " 'ipad website': 10344,\n",
       " 'okay': 14801,\n",
       " 'really': 16974,\n",
       " 'yay': 23583,\n",
       " 'new': 14387,\n",
       " 'app11': 1363,\n",
       " 'kthxbai': 11361,\n",
       " 'okay really': 14804,\n",
       " 'really yay': 17019,\n",
       " 'yay new': 23586,\n",
       " 'new foursquare': 14414,\n",
       " 'foursquare android': 7037,\n",
       " 'android app11': 1028,\n",
       " 'app11 kthxbai': 1364,\n",
       " 'kthxbai sxsw': 11362,\n",
       " 'photo': 15581,\n",
       " 'installed': 9766,\n",
       " 'nice': 14524,\n",
       " 'photo just': 15586,\n",
       " 'just installed': 11080,\n",
       " 'installed sxsw': 9769,\n",
       " 'app really': 1315,\n",
       " 'really nice': 17007,\n",
       " 'enjoying': 6030,\n",
       " 'changes': 3535,\n",
       " '30': 246,\n",
       " 'forward': 7018,\n",
       " 'seeing': 17992,\n",
       " 'sleeves': 18552,\n",
       " 'really enjoying': 16988,\n",
       " 'enjoying changes': 6031,\n",
       " 'changes gowalla': 3537,\n",
       " 'gowalla 30': 8180,\n",
       " '30 android': 247,\n",
       " 'android looking': 1065,\n",
       " 'looking forward': 12358,\n",
       " 'forward seeing': 7030,\n",
       " 'seeing amp': 17994,\n",
       " 'amp foursquare': 923,\n",
       " 'foursquare sleeves': 7043,\n",
       " 'sleeves sxsw': 18554,\n",
       " 'laurieshook': 11562,\n",
       " 'smcdallas': 18634,\n",
       " 'pre': 16041,\n",
       " 'party': 15240,\n",
       " 'wed': 22953,\n",
       " 'hoping': 9167,\n",
       " 'win': 23138,\n",
       " 'resulting': 17327,\n",
       " 'shameless': 18236,\n",
       " 'promotion': 16307,\n",
       " 'chevysmc': 3692,\n",
       " 'rt laurieshook': 17585,\n",
       " 'laurieshook im': 11563,\n",
       " 'im looking': 9473,\n",
       " 'forward smcdallas': 7031,\n",
       " 'smcdallas pre': 18635,\n",
       " 'pre sxsw': 16042,\n",
       " 'sxsw party': 20175,\n",
       " 'party wed': 15299,\n",
       " 'wed hoping': 22955,\n",
       " 'hoping ill': 9169,\n",
       " 'ill win': 9423,\n",
       " 'win ipad': 23144,\n",
       " 'ipad resulting': 10252,\n",
       " 'resulting shameless': 17328,\n",
       " 'shameless promotion': 18237,\n",
       " 'promotion chevysmc': 16308,\n",
       " 'michaelpiliero': 13797,\n",
       " 'rt haha': 17583,\n",
       " 'sxsw michaelpiliero': 20121,\n",
       " 'started': 19051,\n",
       " 'partnerhub': 15234,\n",
       " 'group': 8361,\n",
       " 'groups': 8384,\n",
       " 'presxsw': 16125,\n",
       " 'started austin': 19052,\n",
       " 'austin partnerhub': 1990,\n",
       " 'partnerhub group': 15235,\n",
       " 'group google': 8365,\n",
       " 'google groups': 7913,\n",
       " 'groups presxsw': 8386,\n",
       " 'presxsw great': 16126,\n",
       " 'great idea': 8275,\n",
       " '4sq3': 355,\n",
       " 'looks': 12368,\n",
       " 'like': 11770,\n",
       " 'going': 7685,\n",
       " 'rock': 17490,\n",
       " 'update': 22166,\n",
       " 'push': 16414,\n",
       " 'tonight': 21555,\n",
       " 'keepaustinweird': 11172,\n",
       " 'new 4sq3': 14390,\n",
       " '4sq3 looks': 356,\n",
       " 'looks like': 12376,\n",
       " 'like going': 11802,\n",
       " 'going rock': 7720,\n",
       " 'rock update': 17498,\n",
       " 'update iphone': 22172,\n",
       " 'iphone android': 10525,\n",
       " 'android push': 1084,\n",
       " 'push tonight': 16418,\n",
       " 'tonight sxsw': 21564,\n",
       " 'sxsw keepaustinweird': 20058,\n",
       " 'right': 17417,\n",
       " 'sweeeeet': 19680,\n",
       " 'job': 10942,\n",
       " 'team': 20791,\n",
       " 'right gowalla': 17427,\n",
       " 'gowalla app': 8181,\n",
       " 'app android': 1196,\n",
       " 'android sweeeeet': 1090,\n",
       " 'sweeeeet nice': 19681,\n",
       " 'nice job': 14534,\n",
       " 'job team': 10949,\n",
       " 'team sxsw': 20802,\n",
       " 'leave': 11626,\n",
       " 'vuvuzela': 22646,\n",
       " 'home': 9091,\n",
       " 'smart madebymany': 18600,\n",
       " 'sxsw leave': 20069,\n",
       " 'leave vuvuzela': 11632,\n",
       " 'vuvuzela home': 22647,\n",
       " 'ipad going': 10101,\n",
       " 'going sxsw': 7727,\n",
       " 'sxsw hollergram': 20012,\n",
       " 'mention': 13121,\n",
       " 'ha': 8543,\n",
       " 'line': 11893,\n",
       " 'quotpopupquot': 16742,\n",
       " 'event': 6156,\n",
       " 'planner': 15752,\n",
       " 'eventprofs': 6166,\n",
       " 'pcma': 15373,\n",
       " 'engage365': 6000,\n",
       " 'best rt': 2523,\n",
       " 'rt mention': 17589,\n",
       " 'mention ha': 13350,\n",
       " 'ha line': 8547,\n",
       " 'line ipad2': 11920,\n",
       " 'ipad2 sxsw': 10431,\n",
       " 'sxsw quotpopupquot': 20235,\n",
       " 'quotpopupquot apple': 16743,\n",
       " 'store event': 19217,\n",
       " 'event planner': 6160,\n",
       " 'planner eventprofs': 15754,\n",
       " 'eventprofs pcma': 6167,\n",
       " 'pcma engage365': 15374,\n",
       " 'false': 6460,\n",
       " 'alarm': 804,\n",
       " 'circles': 3760,\n",
       " 'ûòand': 23925,\n",
       " 'probably': 16208,\n",
       " 'link': 11978,\n",
       " 'social': 18679,\n",
       " 'mention false': 13292,\n",
       " 'false alarm': 6461,\n",
       " 'alarm google': 805,\n",
       " 'google circles': 7871,\n",
       " 'circles coming': 3764,\n",
       " 'coming ûòand': 4058,\n",
       " 'ûòand probably': 23926,\n",
       " 'probably link': 16214,\n",
       " 'link google': 12028,\n",
       " 'circles social': 3788,\n",
       " 'social sxsw': 18709,\n",
       " 'weather': 22906,\n",
       " 'greet': 8341,\n",
       " 'sweater': 19678,\n",
       " 'nightapple': 14584,\n",
       " 'putting': 16431,\n",
       " 'quotflash': 16611,\n",
       " 'storequot': 19332,\n",
       " 'downtown': 5684,\n",
       " 'sell': 18045,\n",
       " 'mention great': 13341,\n",
       " 'great weather': 8333,\n",
       " 'weather greet': 22909,\n",
       " 'greet sxsw': 8342,\n",
       " 'sxsw need': 20138,\n",
       " 'need sweater': 14304,\n",
       " 'sweater nightapple': 19679,\n",
       " 'nightapple putting': 14585,\n",
       " 'putting quotflash': 16436,\n",
       " 'quotflash storequot': 16612,\n",
       " 'storequot downtown': 19333,\n",
       " 'downtown sell': 5695,\n",
       " 'sell ipad2': 18050,\n",
       " 'smartcover': 18618,\n",
       " 'ûª': 23842,\n",
       " 'opens': 14922,\n",
       " 'instant': 9774,\n",
       " 'access': 545,\n",
       " 'waited': 22684,\n",
       " 'ipad2 smartcover': 10427,\n",
       " 'smartcover ûª': 18619,\n",
       " 'ûª opens': 23847,\n",
       " 'opens instant': 14925,\n",
       " 'instant access': 9775,\n",
       " 'access waited': 548,\n",
       " 'waited link': 22688,\n",
       " 'link apple': 11986,\n",
       " 'apple sxsw': 1569,\n",
       " 'hooray': 9125,\n",
       " 'ûïmention': 23876,\n",
       " 'opening': 14911,\n",
       " 'popup': 15919,\n",
       " 'hooray rt': 9128,\n",
       " 'rt ûïmention': 17594,\n",
       " 'ûïmention apple': 23878,\n",
       " 'apple opening': 1504,\n",
       " 'opening popup': 14917,\n",
       " 'popup store': 15931,\n",
       " 'store austin': 19186,\n",
       " 'sxsw mention': 20120,\n",
       " 'mention link': 13438,\n",
       " 'wooooo': 23328,\n",
       " 'open': 14873,\n",
       " 'midnight': 13817,\n",
       " 'wooooo ûïmention': 23330,\n",
       " 'store downtown': 19213,\n",
       " 'downtown austin': 5688,\n",
       " 'austin open': 1987,\n",
       " 'open til': 14897,\n",
       " 'til midnight': 21327,\n",
       " 'midnight sxsw': 13819,\n",
       " 'talking': 20731,\n",
       " 'googles': 8094,\n",
       " 'effort': 5916,\n",
       " 'allow': 826,\n",
       " 'systems': 20609,\n",
       " 'bettercloud': 2572,\n",
       " 'ûïmention mention': 23891,\n",
       " 'mention talking': 13664,\n",
       " 'talking link': 20735,\n",
       " 'link googles': 12030,\n",
       " 'googles effort': 8097,\n",
       " 'effort allow': 5917,\n",
       " 'allow users': 829,\n",
       " 'users open': 22334,\n",
       " 'open systems': 14894,\n",
       " 'systems bettercloud': 20610,\n",
       " 'bettercloud sxsw': 2573,\n",
       " '1st': 134,\n",
       " 'stop': 19154,\n",
       " 'chaos': 3546,\n",
       " 'hunt': 9317,\n",
       " 'java': 10898,\n",
       " 'spy': 18984,\n",
       " 'chance': 3516,\n",
       " 'link rt': 12087,\n",
       " 'mention 1st': 13126,\n",
       " '1st stop': 139,\n",
       " 'stop sxsw': 19162,\n",
       " 'sxsw chaos': 19839,\n",
       " 'chaos amp': 3547,\n",
       " 'amp mention': 950,\n",
       " 'mention hunt': 13386,\n",
       " 'hunt austin': 9318,\n",
       " 'austin java': 1977,\n",
       " 'java spy': 10899,\n",
       " 'spy game': 18985,\n",
       " 'game chance': 7328,\n",
       " 'chance win': 3519,\n",
       " 'omfg': 14829,\n",
       " 'heard': 8825,\n",
       " 'apples': 1607,\n",
       " 'pics': 15644,\n",
       " 'omfg rt': 14830,\n",
       " 'mention heard': 13363,\n",
       " 'heard apples': 8828,\n",
       " 'apples popup': 1618,\n",
       " 'austin pics': 1993,\n",
       " 'pics gowalla': 15647,\n",
       " 'gowalla link': 8185,\n",
       " 'link sxsw': 12101,\n",
       " 'sxsw ipad2': 20043,\n",
       " 'attending': 1881,\n",
       " 'headaches': 8777,\n",
       " 'attending mention': 1885,\n",
       " 'mention ipad': 13406,\n",
       " 'ipad design': 10044,\n",
       " 'design headaches': 5093,\n",
       " 'headaches sxsw': 8784,\n",
       " 'sxsw link': 20075,\n",
       " 'power': 15997,\n",
       " 'sxswi': 20513,\n",
       " 'check mention': 3636,\n",
       " 'mention mention': 13467,\n",
       " 'mention amp': 13148,\n",
       " 'mention line': 13437,\n",
       " 'line ipad': 11919,\n",
       " 'ipad austin': 9992,\n",
       " 'austin power': 1996,\n",
       " 'power sxswi': 16006,\n",
       " 'sxswi sxsw': 20531,\n",
       " 'bands': 2277,\n",
       " 'food': 6947,\n",
       " 'art': 1756,\n",
       " 'ice': 9344,\n",
       " 'cream': 4588,\n",
       " 'nifty': 14563,\n",
       " 'interactive': 9816,\n",
       " 'maps': 12810,\n",
       " 'mention come': 13225,\n",
       " 'come party': 4011,\n",
       " 'party google': 15261,\n",
       " 'google tonight': 8044,\n",
       " 'link bands': 11992,\n",
       " 'bands food': 2279,\n",
       " 'food art': 6949,\n",
       " 'art ice': 1760,\n",
       " 'ice cream': 9345,\n",
       " 'cream nifty': 4589,\n",
       " 'nifty interactive': 14564,\n",
       " 'interactive maps': 9826,\n",
       " 'holla': 9071,\n",
       " 'butt': 3171,\n",
       " 'holla rt': 9072,\n",
       " 'mention google': 13337,\n",
       " 'google party': 7971,\n",
       " 'party best': 15244,\n",
       " 'best butt': 2507,\n",
       " 'butt sxsw': 3172,\n",
       " 'case': 3376,\n",
       " 'phone': 15535,\n",
       " 'love mention': 12478,\n",
       " 'mention iphone': 13408,\n",
       " 'iphone case': 10549,\n",
       " 'case sxsw': 3394,\n",
       " 'sxsw phone': 20184,\n",
       " 'phone fail': 15546,\n",
       " 'post': 15974,\n",
       " 'makes': 12712,\n",
       " 'easy': 5871,\n",
       " 'connect': 4237,\n",
       " 'networks': 14381,\n",
       " 'new post': 14442,\n",
       " 'post mention': 15979,\n",
       " 'app makes': 1293,\n",
       " 'makes easy': 12714,\n",
       " 'easy connect': 5873,\n",
       " 'connect social': 4239,\n",
       " 'social networks': 18699,\n",
       " 'networks people': 14383,\n",
       " 'people meet': 15441,\n",
       " 'meet link': 13075,\n",
       " 'behaving': 2469,\n",
       " 'today': 21473,\n",
       " 'crashes': 4557,\n",
       " 'yesterday': 23687,\n",
       " 'ridiculous': 17405,\n",
       " 'nice mention': 14541,\n",
       " 'app behaving': 1206,\n",
       " 'behaving today': 2470,\n",
       " 'today crashes': 21486,\n",
       " 'crashes yesterday': 4560,\n",
       " 'yesterday ridiculous': 23692,\n",
       " 'ridiculous sxsw': 17408,\n",
       " 'hey': 8919,\n",
       " 'fans': 6491,\n",
       " 'peek': 15394,\n",
       " 'space': 18857,\n",
       " 'thats': 21054,\n",
       " 'slated': 18544,\n",
       " 'tomorrow': 21528,\n",
       " 'nice rt': 14546,\n",
       " 'mention hey': 13369,\n",
       " 'hey apple': 8920,\n",
       " 'apple fans': 1446,\n",
       " 'fans peek': 6493,\n",
       " 'peek space': 15395,\n",
       " 'space thats': 18866,\n",
       " 'thats slated': 21073,\n",
       " 'slated popup': 18545,\n",
       " 'popup sxsw': 15934,\n",
       " 'sxsw apple': 19778,\n",
       " 'store tomorrow': 19311,\n",
       " 'tomorrow link': 21538,\n",
       " 'thing': 21158,\n",
       " 'doing': 5520,\n",
       " 'earth': 5845,\n",
       " 'face': 6400,\n",
       " 'company': 4093,\n",
       " 'sxwsi': 20582,\n",
       " 'thing mention': 21172,\n",
       " 'mention doing': 13266,\n",
       " 'doing great': 5526,\n",
       " 'great great': 8274,\n",
       " 'great earth': 8265,\n",
       " 'earth face': 5847,\n",
       " 'face google': 6402,\n",
       " 'google company': 7872,\n",
       " 'company love': 4103,\n",
       " 'sxsw sxwsi': 20347,\n",
       " 'thanks': 21024,\n",
       " 'speech': 18907,\n",
       " 'apps': 1657,\n",
       " 'showcased': 18374,\n",
       " 'conf': 4172,\n",
       " 'sxswh': 20510,\n",
       " 'sxsh': 19735,\n",
       " 'thanks mention': 21041,\n",
       " 'mention new': 13486,\n",
       " 'new speech': 14450,\n",
       " 'speech ipad': 18908,\n",
       " 'ipad apps': 9987,\n",
       " 'apps showcased': 1692,\n",
       " 'showcased sxsw': 18376,\n",
       " 'sxsw conf': 19860,\n",
       " 'conf link': 4175,\n",
       " 'link sxswh': 12103,\n",
       " 'sxswh sxsh': 20512,\n",
       " 'does': 5475,\n",
       " 'provide': 16334,\n",
       " 'chargers': 3571,\n",
       " 'ive': 10847,\n",
       " 'changed': 3527,\n",
       " 'mind': 13848,\n",
       " 'year': 23603,\n",
       " 'mention sxsw': 13655,\n",
       " 'sxsw does': 19889,\n",
       " 'does provide': 5490,\n",
       " 'provide iphone': 16335,\n",
       " 'iphone chargers': 10555,\n",
       " 'chargers ive': 3572,\n",
       " 'ive changed': 10850,\n",
       " 'changed mind': 3531,\n",
       " 'mind going': 13850,\n",
       " 'going year': 7732,\n",
       " 'xmas': 23549,\n",
       " 'shiny': 18290,\n",
       " 'garyvee': 7377,\n",
       " 'book': 2817,\n",
       " 'stores': 19335,\n",
       " 'christmas': 3738,\n",
       " 'nerds': 14342,\n",
       " 'xmas rt': 23550,\n",
       " 'mention shiny': 13622,\n",
       " 'shiny new': 18291,\n",
       " 'new mention': 14435,\n",
       " 'mention apps': 13158,\n",
       " 'apps new': 1686,\n",
       " 'new garyvee': 14416,\n",
       " 'garyvee book': 7378,\n",
       " 'book popup': 2825,\n",
       " 'popup ipad': 15924,\n",
       " 'ipad stores': 10287,\n",
       " 'stores sxsw': 19340,\n",
       " 'sxsw christmas': 19843,\n",
       " 'christmas nerds': 3739,\n",
       " 'yai': 23569,\n",
       " 'ubersocial': 22029,\n",
       " 'includes': 9623,\n",
       " 'uberguide': 22027,\n",
       " 'sponsored': 18956,\n",
       " 'cont': 4285,\n",
       " 'yai rt': 23570,\n",
       " 'new ubersocial': 14459,\n",
       " 'ubersocial iphone': 22030,\n",
       " 'app store': 1327,\n",
       " 'store includes': 19239,\n",
       " 'includes uberguide': 9625,\n",
       " 'uberguide sxsw': 22028,\n",
       " 'sxsw sponsored': 20309,\n",
       " 'sponsored cont': 18958,\n",
       " 'cont link': 4286,\n",
       " 'fast': 6541,\n",
       " 'fun': 7218,\n",
       " 'future': 7261,\n",
       " 'presenting': 16105,\n",
       " 'search': 17937,\n",
       " 'local': 12229,\n",
       " 'mobile': 13948,\n",
       " 'fast fun': 6548,\n",
       " 'fun amp': 7219,\n",
       " 'amp future': 927,\n",
       " 'future mention': 7274,\n",
       " 'google presenting': 7984,\n",
       " 'presenting sxsw': 16109,\n",
       " 'sxsw search': 20271,\n",
       " 'search local': 17945,\n",
       " 'local mobile': 12232,\n",
       " 'headline': 8803,\n",
       " 'quotipad': 16670,\n",
       " 'musthave': 14198,\n",
       " 'gadget': 7303,\n",
       " 'sxswquot': 20559,\n",
       " 'hmm': 9036,\n",
       " 'seen': 18007,\n",
       " 'headline quotipad': 8804,\n",
       " 'quotipad musthave': 16672,\n",
       " 'musthave gadget': 14201,\n",
       " 'gadget sxswquot': 7306,\n",
       " 'sxswquot hmm': 20562,\n",
       " 'hmm seen': 9039,\n",
       " 'seen coming': 18012,\n",
       " 'coming link': 4046,\n",
       " 'link gadget': 12022,\n",
       " 'quotgoogle': 16619,\n",
       " 'launched': 11527,\n",
       " 'checkins': 3670,\n",
       " 'month': 14065,\n",
       " 'agoquot': 738,\n",
       " 'ins': 9722,\n",
       " 'ok': 14795,\n",
       " 'outs': 15024,\n",
       " 'bizzy': 2691,\n",
       " 'mention quotgoogle': 13553,\n",
       " 'quotgoogle launched': 16623,\n",
       " 'launched checkins': 11528,\n",
       " 'checkins month': 3672,\n",
       " 'month agoquot': 14066,\n",
       " 'agoquot check': 739,\n",
       " 'check ins': 3626,\n",
       " 'ins ok': 9723,\n",
       " 'ok check': 14796,\n",
       " 'check outs': 3640,\n",
       " 'outs future': 15025,\n",
       " 'future sxsw': 7278,\n",
       " 'sxsw bizzy': 19813,\n",
       " 'tweetquot': 21959,\n",
       " 'quotthink': 16800,\n",
       " 'speakquot': 18892,\n",
       " 'mark': 12899,\n",
       " 'belinsky': 2481,\n",
       " '911tweets': 481,\n",
       " 'panel': 15143,\n",
       " 'ûïmention quotgoogle': 23897,\n",
       " 'quotgoogle tweetquot': 16630,\n",
       " 'tweetquot new': 21960,\n",
       " 'new quotthink': 14446,\n",
       " 'quotthink speakquot': 16801,\n",
       " 'speakquot mark': 18894,\n",
       " 'mark belinsky': 12900,\n",
       " 'belinsky 911tweets': 2482,\n",
       " '911tweets panel': 482,\n",
       " 'panel sxsw': 15161,\n",
       " 'kawasaki': 11165,\n",
       " 'quotnot': 16715,\n",
       " 'cs': 4679,\n",
       " 'lewis': 11713,\n",
       " 'level': 11706,\n",
       " 'reasoning': 17035,\n",
       " 'continued': 4319,\n",
       " 'existence': 6291,\n",
       " 'evidence': 6201,\n",
       " 'godquot': 7657,\n",
       " 'bawling': 2398,\n",
       " 'kawasaki quotnot': 11169,\n",
       " 'quotnot cs': 16716,\n",
       " 'cs lewis': 4680,\n",
       " 'lewis level': 11714,\n",
       " 'level reasoning': 11710,\n",
       " 'reasoning apples': 17036,\n",
       " 'apples continued': 1609,\n",
       " 'continued existence': 4320,\n",
       " 'existence evidence': 6292,\n",
       " 'evidence existence': 6202,\n",
       " 'existence godquot': 6293,\n",
       " 'godquot bawling': 7658,\n",
       " 'bawling sxsw': 2399,\n",
       " 'quotpagemaker': 16725,\n",
       " 'saved': 17736,\n",
       " 'applequot': 1602,\n",
       " 'jwtatl': 11155,\n",
       " 'enchantment': 5978,\n",
       " 'kawasaki quotpagemaker': 11170,\n",
       " 'quotpagemaker saved': 16726,\n",
       " 'saved applequot': 17738,\n",
       " 'applequot oh': 1603,\n",
       " 'oh days': 14777,\n",
       " 'sxsw jwtatl': 20057,\n",
       " 'jwtatl enchantment': 11156,\n",
       " 'enchantment mention': 5980,\n",
       " '2s': 215,\n",
       " 'thoughts': 21276,\n",
       " 'japan': 10878,\n",
       " 'apac': 1182,\n",
       " 'regions': 17130,\n",
       " 'dealing': 4921,\n",
       " 'earthquake': 5855,\n",
       " 'tsunami': 21849,\n",
       " 'trauma': 21725,\n",
       " 'apple ipad': 1473,\n",
       " 'ipad 2s': 9964,\n",
       " '2s great': 225,\n",
       " 'great thoughts': 8321,\n",
       " 'thoughts japan': 21277,\n",
       " 'japan apac': 10879,\n",
       " 'apac regions': 1183,\n",
       " 'regions dealing': 17131,\n",
       " 'dealing earthquake': 4922,\n",
       " 'earthquake amp': 5856,\n",
       " 'amp tsunami': 984,\n",
       " 'tsunami trauma': 21850,\n",
       " 'trauma sxswi': 21726,\n",
       " 'schools': 17886,\n",
       " 'marketing': 12918,\n",
       " 'experts': 6341,\n",
       " 'apple schools': 1534,\n",
       " 'schools marketing': 17887,\n",
       " 'marketing experts': 12923,\n",
       " 'experts link': 6343,\n",
       " 'temporary': 20942,\n",
       " 'def': 4970,\n",
       " 'tent': 20962,\n",
       " 'powerhouse': 16009,\n",
       " 'gym': 8529,\n",
       " 'temporary apple': 20943,\n",
       " 'store def': 19207,\n",
       " 'def tent': 4973,\n",
       " 'tent powerhouse': 20963,\n",
       " 'powerhouse gym': 16010,\n",
       " 'gym sxsw': 8533,\n",
       " '6th': 433,\n",
       " 'congress': 4217,\n",
       " '10000': 24,\n",
       " 'happy': 8673,\n",
       " 'hipsters': 9002,\n",
       " 'store 6th': 19179,\n",
       " '6th congress': 436,\n",
       " 'congress sxsw': 4233,\n",
       " 'sxsw 10000': 19741,\n",
       " '10000 happy': 25,\n",
       " 'happy hipsters': 8679,\n",
       " 'wins': 23193,\n",
       " 'support': 19589,\n",
       " 'launch': 11486,\n",
       " 'apple wins': 1582,\n",
       " 'wins sxsw': 23200,\n",
       " 'link opening': 12071,\n",
       " 'opening temporary': 14921,\n",
       " 'temporary store': 20949,\n",
       " 'austin support': 2010,\n",
       " 'support ipad2': 19593,\n",
       " 'ipad2 launch': 10406,\n",
       " 'launch good': 11500,\n",
       " 'trending': 21744,\n",
       " 'nerdy': 14350,\n",
       " 'austin trending': 2021,\n",
       " 'trending today': 21747,\n",
       " 'today fun': 21489,\n",
       " 'fun sxsw': 7239,\n",
       " 'sxsw nerdy': 20142,\n",
       " 'nerdy nerds': 14351,\n",
       " 'christian': 3736,\n",
       " 'devs': 5221,\n",
       " 'want': 22757,\n",
       " 'talk': 20698,\n",
       " 'maybe': 12981,\n",
       " 'wk': 23248,\n",
       " ...}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ShA6D8jKH0N5"
   },
   "source": [
    "### 11. Find out how many Positive and Negative emotions are there.\n",
    "\n",
    "Hint: Use value_counts on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7LAl5pzH0N6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive emotion    2672\n",
       "Negative emotion     519\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"is_there_an_emotion_directed_at_a_brand_or_product\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUvgj0FoH0N9"
   },
   "source": [
    "### 12. Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'Label'\n",
    "\n",
    "Hint: use map on that column and give labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YftKwFv7H0N9"
   },
   "outputs": [],
   "source": [
    "data[\"is_there_an_emotion_directed_at_a_brand_or_product\"] = data[\"is_there_an_emotion_directed_at_a_brand_or_product\"].map({'Positive emotion':1, 'Negative emotion':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2672\n",
       "0     519\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"is_there_an_emotion_directed_at_a_brand_or_product\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YErwYLCH0N_"
   },
   "source": [
    "### 13. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets and display shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNkwrGgEH0OA"
   },
   "outputs": [],
   "source": [
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = data['text']\n",
    "y = data['is_there_an_emotion_directed_at_a_brand_or_product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUHrfDCyH0PP"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3H4k_lVZH0PS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2233,), (2233,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3H4k_lVZH0PS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((958,), (958,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5nlCuaaH0OD"
   },
   "source": [
    "## 14. **Predicting the sentiment:**\n",
    "\n",
    "\n",
    "### Use (i) Naive Bayes and (ii) Logistic Regression and print their accuracy scores for predicting the sentiment of the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AbVYssaH0OE"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktXrLhmOH0Of"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "\n",
    "# create document-term matrices\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clv2X0kKH0Ok"
   },
   "outputs": [],
   "source": [
    "nb_clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.fit(x_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = nb_clf.predict(x_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9493954321540529"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = nb_clf.predict(x_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8674321503131524"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K86LRMfdH0Ou"
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.fit(x_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = lr_clf.predict(x_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9816390506045678"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = lr_clf.predict(x_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8716075156576201"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sw-0B33tH0Ox"
   },
   "source": [
    "### 15. Create a function called `tokenize_predict` which can take count vectorizer object as input, create document term matrix out of x_train & x_test, build and train a model using dtm created and print the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okCTOs1TH0Oy"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "def tokenize_predict(vect):\n",
    "    x_train_dtm = vect.fit_transform(x_train)\n",
    "    print('Features: ', x_train_dtm.shape[1])\n",
    "    x_test_dtm = vect.transform(x_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(x_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(x_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxZ8jfPEH0O0"
   },
   "source": [
    "### 16. Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score\n",
    "\n",
    "Hint: vect = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdCyAN_IH0O0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  24045\n",
      "Accuracy:  0.8684759916492694\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "#ngram range = (1,2) and default values for all other parameters\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "tokenize_predict(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axepytmgH0O4"
   },
   "source": [
    "### 17. Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HToGkq7vH0O4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  18629\n",
      "Accuracy:  0.8747390396659708\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words\n",
    "#ngram range = (1,2), stop_word='english' and default values for all other parameters\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,2),stop_words='english')\n",
    "tokenize_predict(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOIlJRxoH0O7"
   },
   "source": [
    "### 18. Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HToGkq7vH0O4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  300\n",
      "Accuracy:  0.7964509394572025\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words and only keep 300 features\n",
    "#ngram range = (1,2), stop_word='english', max_features=300 and default values for all other parameters\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,2),stop_words='english', max_features=300)\n",
    "tokenize_predict(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2KZNWVkH0PA"
   },
   "source": [
    "### 19. Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3v9XD082H0PB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  15000\n",
      "Accuracy:  0.8663883089770354\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and limit the number of features to 15000\n",
    "#ngram range = (1,2), max_features=15000 and default values for all other parameters\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,2), max_features=15000)\n",
    "tokenize_predict(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We3JK_SRH0PO"
   },
   "source": [
    "### 20. Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VltP3aeMvrW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  7149\n",
      "Accuracy:  0.8705636743215032\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and only include terms that appear at least 2 times\n",
    "#ngram range = (1,2), min_df=2 and default values for all other parameters\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
    "tokenize_predict(vect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
