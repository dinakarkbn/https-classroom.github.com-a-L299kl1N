{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model\n",
    "- Word2Vec is a group of related models that are used to produce word embedding.\n",
    "- These models are shallow 2-layer neural networks that are trained to reconstructlinguistic context of words\n",
    "- Word2Vec uses either of Continuous Bag of Word (CBOW) model or skip-gram model\n",
    "- Word2Vec is a method to construct word embedding using two methods, both invoving Neural Networks.The two methods are CBOW and Skip-Gram\n",
    "\n",
    "- CBOW : Try to predict a word given the surrounding context of the word (e.g. location+/-2)\n",
    "    - This method takes the context of each word as the input and tries to predict the word corresponding to the context\n",
    "    - e.g. It __was a cat that made__ all the noise\n",
    "    - (was->cat), (a->cat), (that->cat), (made->cat)\n",
    "- Skip-Gram : Try to model the contextual words (e.g. location+/-2) given a particular word\n",
    "    - It tries to predict the source context words(surrounding words) given a target word (the center word)\n",
    "    - It usually tries to achieve the reverse of what the CBOW model does\n",
    "    - e.g. It __was a cat that made__ all the noise\n",
    "    - (cat->was), (cat->a), (cat->that), (cat->made)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/AIML_Learning_IPython_NB/StatisticalNLP/glove.6B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors # Gensim contains word2vec models and processing tools\n",
    "\n",
    "path = 'E:/AIML_Learning_IPython_NB/StatisticalNLP/'\n",
    "glove_file = datapath(path + 'glove.6B.50d.txt') # This is a GloVe model\n",
    "print(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/AIML_Learning_IPython_NB/StatisticalNLP/word2vec.glove.6B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "tmp_file = get_tmpfile(path + 'word2vec.glove.6B.50d.txt')\n",
    "print(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, tmp_file)  # Converting the GloVe file into a Word2Vec file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(tmp_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "[ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
      "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
      "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
      " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
      " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
      "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
      " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
      " -0.51042 ]\n"
     ]
    }
   ],
   "source": [
    "# Check out what the embedding looks like\n",
    "wordEmbed=model['king']\n",
    "print(wordEmbed.shape)\n",
    "print(wordEmbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amit is NOT in the model\n"
     ]
    }
   ],
   "source": [
    "# What happens if a word it out of the dictionary?\n",
    "\n",
    "word = 'Amit'\n",
    "if word in model:\n",
    "    print('{0} is in the model'.format(word))\n",
    "else:\n",
    "    print('{0} is NOT in the model'.format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amit is in the model\n"
     ]
    }
   ],
   "source": [
    "word = 'amit'\n",
    "if word in model:\n",
    "    print('{0} is in the model'.format(word))\n",
    "else:\n",
    "    print('{0} is NOT in the model'.format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words which are most similar to the word - boy :\n",
      "\n",
      " [('girl', 0.932719886302948), ('woman', 0.8596120476722717), ('man', 0.8564430475234985), ('kid', 0.8192578554153442), ('mother', 0.8179756999015808), ('teenage', 0.8029857277870178), ('baby', 0.8001461029052734), ('dad', 0.785053014755249), ('her', 0.783680260181427), ('old', 0.7815377116203308)]\n",
      "Words which are most similar to the word - cat :\n",
      "\n",
      " [('dog', 0.9218006134033203), ('rabbit', 0.8487820625305176), ('monkey', 0.8041081428527832), ('rat', 0.7891963124275208), ('cats', 0.7865270376205444), ('snake', 0.7798910737037659), ('dogs', 0.7795814871788025), ('pet', 0.7792249917984009), ('mouse', 0.7731667757034302), ('bite', 0.7728800773620605)]\n",
      "Words which are most similar to the word - rice :\n",
      "\n",
      " [('wheat', 0.7770208120346069), ('corn', 0.7385713458061218), ('vegetables', 0.7257848978042603), ('grain', 0.7143878936767578), ('beans', 0.7025899887084961), ('maize', 0.6951528787612915), ('fresh', 0.6852614879608154), ('fruit', 0.6797670125961304), ('food', 0.6762567758560181), ('vegetable', 0.6745879650115967)]\n",
      "Words which are most similar to the word - tennis :\n",
      "\n",
      " [('volleyball', 0.8184806108474731), ('badminton', 0.8163268566131592), ('tournament', 0.8126396536827087), ('wimbledon', 0.8122076988220215), ('championships', 0.7815724015235901), ('masters', 0.7800596952438354), ('golf', 0.7766085863113403), ('indoor', 0.7727329730987549), ('wta', 0.7600159645080566), ('ladies', 0.7552402019500732)]\n",
      "Words which are most similar to the word - beauty :\n",
      "\n",
      " [('beautiful', 0.8014683723449707), ('glamour', 0.7638750076293945), ('charm', 0.7530314922332764), ('wonderful', 0.7496905326843262), ('lovely', 0.7171888947486877), ('life', 0.7141240835189819), ('passion', 0.7124689817428589), ('elegance', 0.7094277143478394), ('image', 0.7029639482498169), ('love', 0.6999226808547974)]\n"
     ]
    }
   ],
   "source": [
    "# Words which are most similar to a given word\n",
    "\n",
    "print(\"Words which are most similar to the word - boy :\\n\\n\",model.most_similar(positive=['boy']))\n",
    "print(\"Words which are most similar to the word - cat :\\n\\n\",model.most_similar(positive=['cat']))\n",
    "print(\"Words which are most similar to the word - rice :\\n\\n\",model.most_similar(positive=['rice']))\n",
    "print(\"Words which are most similar to the word - tennis :\\n\\n\",model.most_similar(positive=['tennis']))\n",
    "print(\"Words which are most similar to the word - beauty :\\n\\n\",model.most_similar(positive=['beauty']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('toddler', 0.7926537990570068), ('teenage', 0.7791370749473572), ('girls', 0.7590369582176208), ('12-year-old', 0.7517068386077881), ('girlfriend', 0.7495955228805542), ('baby', 0.7431076169013977), ('teen', 0.7420293092727661), ('9-year', 0.7407599091529846), ('14-year-old', 0.7330296039581299), ('orphan', 0.726045548915863)]\n"
     ]
    }
   ],
   "source": [
    "# Most like X but unlike Y\n",
    "# words which are similar to boy and girl but different from man\n",
    "\n",
    "print(model.most_similar(positive=['boy', 'girl'], negative=['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "chess\n",
      "beauty\n"
     ]
    }
   ],
   "source": [
    "# does'nt match utility gives the odd one out of a given set of words.\n",
    "#ie, which words are closest and which words are different\n",
    "\n",
    "print(model.doesnt_match(\"boy girl dog man\".split()))\n",
    "print(model.doesnt_match(\"cricket tennis rugby hockey golf carroms chess football\".split()))\n",
    "print(model.doesnt_match(\"beauty wise intelligent smart skill quick\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8860338\n",
      "0.6264735\n",
      "0.906528\n",
      "0.27783552\n"
     ]
    }
   ],
   "source": [
    "# similarity method gives the cosine similarity between two words\n",
    "print(model.similarity('woman', 'man'))\n",
    "print(model.similarity('woman', 'beauty'))\n",
    "print(model.similarity('woman', 'girl')) #most similar\n",
    "print(model.similarity('woman', 'clock')) #least similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 0.932719886302948), ('woman', 0.8596120476722717), ('man', 0.8564430475234985), ('kid', 0.8192578554153442), ('mother', 0.8179756999015808), ('teenage', 0.8029857277870178), ('baby', 0.8001461029052734), ('dad', 0.785053014755249), ('her', 0.783680260181427), ('old', 0.7815377116203308)]\n",
      "[('pig', 0.8290977478027344), ('cows', 0.8151975870132446), ('sheep', 0.8077231049537659), ('bovine', 0.801677942276001), ('cattle', 0.7895811200141907), ('pigs', 0.7748672366142273), ('mad', 0.7608286142349243), ('chickens', 0.7410693168640137), ('cloned', 0.7392406463623047), ('bird', 0.7354094386100769)]\n"
     ]
    }
   ],
   "source": [
    "# similar_by_word is same as most_similar\n",
    "print(model.similar_by_word('boy'))\n",
    "print(model.similar_by_word('cow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('man', 0.8706066608428955), ('father', 0.8266595602035522), ('who', 0.8227341771125793), ('woman', 0.8118484616279602), ('death', 0.7955310344696045), ('another', 0.7886117696762085), ('whose', 0.7844259142875671), ('old', 0.7815861701965332), ('victim', 0.7748767137527466), ('him', 0.7725713849067688)] \n",
      "\n",
      "[('japan', 0.9243350028991699), ('tokyo', 0.8630704879760742), ('japanese', 0.8449169397354126), ('china', 0.8107869029045105), ('asian', 0.8040996193885803), ('singapore', 0.7758235931396484), ('shanghai', 0.7707607746124268), ('kong', 0.7669796943664551), ('korea', 0.7546433210372925), ('asia', 0.7535871267318726)]\n"
     ]
    }
   ],
   "source": [
    "# similar_by_vector is used to get word anologies of two pairs of words\n",
    "#e.g. king :: queen is as man :: woman\n",
    "#e.g. india :: delhi is as japan :: tokyo\n",
    "\n",
    "print(model.similar_by_vector(model['king'] - model['queen'] + model['woman']), '\\n') # man has highest similarity\n",
    "print(model.similar_by_vector(model['india'] - model['delhi'] + model['tokyo'])) # japan has highest similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('man', 0.8573544025421143), ('who', 0.8164044618606567), ('father', 0.8128570318222046), ('death', 0.7859841585159302), ('another', 0.7801331281661987), ('whose', 0.7746530771255493), ('old', 0.7679885029792786), ('victim', 0.7661792039871216), ('him', 0.7624202966690063), ('thought', 0.7570509910583496)]\n"
     ]
    }
   ],
   "source": [
    "# word analogies can also be achievedd using most_similar function\n",
    "\n",
    "print(model.most_similar(positive=['king', 'woman'], negative=['queen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('japanese', 0.7138170599937439), ('china', 0.6920005679130554), ('korea', 0.679780125617981), ('tokyo', 0.6514694690704346), ('korean', 0.6441376209259033), ('indian', 0.6359708905220032), ('chinese', 0.6258283257484436), ('britain', 0.6247069835662842), ('greece', 0.6245750188827515), ('europe', 0.6159744262695312)]\n"
     ]
    }
   ],
   "source": [
    "# word analogies can also be achieved using most_similar function\n",
    "\n",
    "print(model.most_similar(positive=['india', 'japan'], negative=['thailand']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('indies', 0.8089947700500488), ('twenty20', 0.8088221549987793), ('rugby', 0.8044811487197876), ('cricketers', 0.7978312373161316), ('england', 0.7800652980804443), ('wc2003', 0.7499520778656006), ('wc2003-wis', 0.7357773780822754), ('bowling', 0.7342497110366821), ('mcc', 0.7253372669219971), ('zealand', 0.7152737379074097)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['cricket']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('football', 0.8488245606422424), ('cricket', 0.8044811487197876), ('soccer', 0.7976311445236206), ('club', 0.7833290100097656), ('league', 0.7731512784957886), ('sevens', 0.7537116408348083), ('hockey', 0.7334932088851929), ('england', 0.7309114933013916), ('captained', 0.7293442487716675), ('wales', 0.7291689515113831)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['rugby']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
